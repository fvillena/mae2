{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import datasets # Biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural\n",
    "import es_core_news_sm # Modelo Spacy de procesamiento de lenguaje natural en espa√±ol\n",
    "import spacy # Biblioteca de procesamiento de lenguaje natural\n",
    "import pandas as pd # Biblioteca de manejo de conjuntos de datos\n",
    "import re # M√≥dulo de expresiones regulares\n",
    "import tokenizers # Biblioteca de tokenizaci√≥n de texto\n",
    "import nltk # Biblioteca de procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ó Datasets\n",
    "\n",
    "ü§ó (HuggingFace) Datasets es una biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural que se destaca por la simplicidad de sus m√©todos y el gran repositorio ü§ó Hub que contiene muchos conjuntos de datos libres para descargar s√≥lo con una linea de Python.\n",
    "\n",
    "En nuestro curso trabajaremos con `spanish_diagnostics`, un conjunto de datos de nuestro grupo investigaci√≥n PLN@CMM que contiene textos de sospechas diagn√≥sticas de la lista de espera chilena y est√° etiquetado con el destino de la interconsulta; este destino puede ser `dental` o `no_dental`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset spanish_diagnostics (C:\\Users\\ville\\.cache\\huggingface\\datasets\\spanish_diagnostics\\default\\0.0.0\\45c176cea64580ea9631f78c2867a657ede368597681e5337e9f1c976e4e84ff)\n"
     ]
    }
   ],
   "source": [
    "spanish_diagnostics = datasets.load_dataset('fvillena/spanish_diagnostics') # Con esta linea descargamos el conjunto de datos completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro conjunto de datos cuenta con 2 particiones, una partici√≥n `train` y otra `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 70000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta clase utilizaremos la partici√≥n `train` del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acceder facilmente a atributos de nuestro `Dataset`.\n",
    "\n",
    "- `shape`: Tal como en muchas otras bibliotecas de python este atributo contiene la forma de nuestro conjunto de datos con la sintaxis `(filas, columnas)`.\n",
    "- `column_names`: Este atributo contiene el nombre de las caracter√≠sticas que tiene nuestro conjunto de datos. En nuestro caso tenemos una caracter√≠stica `text`, la cual contiene la hip√≥tesis diagn√≥stica del conjunto de datos y `label` que contiene el destino al cual fue referido.\n",
    "- `features`: Este atributo nos describe la clase a la que pertenece cada una de las caracter√≠sticas. En nuestro caso `text` es un `string` y `label` es del tipo `ClassLabel` con 2 clases con nombre `not_dental` y `dental`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_dental', 'dental'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como en muchas otras clases de datos en Python podemos acceder a subconjuntos de datos a trav√©s de sus √≠ndices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': '- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUSI√ìN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [1, 0, 0],\n",
       " 'text': ['- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUSI√ìN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR',\n",
       "  'OBTRUCCION FOSA NASAL DERECHA',\n",
       "  'Perturbaci√≥n de la actividad y de la atenci√≥n Trastorno def√≠cit atencional']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [0, 0, 1],\n",
       " 'text': ['OBTRUCCION FOSA NASAL DERECHA',\n",
       "  'M7 PROLAPSO VAGINAL PARED ANTERIOR G11 G 111 ALGIA PELVICA HTA CRONICA',\n",
       "  'pieza n 3.4 tratada endodonticamente, restaurada con ionomero y resina compuesta. Necesita protesis fija por gran pNrdida coronaria']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][1,3,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n podemos acceder a cada una de las caracter√≠sticas por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUSI√ìN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR',\n",
       " 'OBTRUCCION FOSA NASAL DERECHA',\n",
       " 'Perturbaci√≥n de la actividad y de la atenci√≥n Trastorno def√≠cit atencional']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"]['text'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con ü§ó Datasets tambi√©n podemos trabajar en otras bibliotecas, como por ejemplo importar el conjunto de datos en Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_diagnostics_train_df = pd.DataFrame(spanish_diagnostics[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>OBTRUCCION FOSA NASAL DERECHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Perturbaci√≥n de la actividad y de la atenci√≥n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>M7 PROLAPSO VAGINAL PARED ANTERIOR G11 G 111 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PIEZA 3 CARIES DENTINARIA PROFUNDA PROXIMA A C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>1</td>\n",
       "      <td>DM1 Evaluaci√≥n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>1</td>\n",
       "      <td>ABCESO SUBMUCOSO PIEZA 2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0</td>\n",
       "      <td>Pbs Inmunodeficiencia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0</td>\n",
       "      <td>QUISTE SINOVIAL DEL HUECO POPLITEO, DE BAKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0</td>\n",
       "      <td>MALLET FINGE D 1 MANO DERECHA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0          1  - ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUS...\n",
       "1          0                      OBTRUCCION FOSA NASAL DERECHA\n",
       "2          0  Perturbaci√≥n de la actividad y de la atenci√≥n ...\n",
       "3          0  M7 PROLAPSO VAGINAL PARED ANTERIOR G11 G 111 A...\n",
       "4          1  PIEZA 3 CARIES DENTINARIA PROFUNDA PROXIMA A C...\n",
       "...      ...                                                ...\n",
       "69995      1                                     DM1 Evaluaci√≥n\n",
       "69996      1                         ABCESO SUBMUCOSO PIEZA 2.6\n",
       "69997      0                              Pbs Inmunodeficiencia\n",
       "69998      0       QUISTE SINOVIAL DEL HUECO POPLITEO, DE BAKER\n",
       "69999      0                      MALLET FINGE D 1 MANO DERECHA\n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que nuestro conjunto de datos tiene sus clases balanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    35034\n",
       "0    34966\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos localmente un conjunto de datos y queremos importarlo a ü§ó Datasets tambi√©n podemos hacerlo. Aqu√≠ importamos el conjunto de datos desde un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-68d4ea5b370697bf\n",
      "Reusing dataset csv (C:\\Users\\ville\\.cache\\huggingface\\datasets\\csv\\default-68d4ea5b370697bf\\0.0.0\\2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['diagnostic', 'is_dental'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_dataset('csv', data_files='spanish_diagnostics/spanish_diagnostics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n\n",
    "\n",
    "Una de las tareas que podemos realizar sobre las caracter√≠sticas no estructuradas de texto es la normalizaci√≥n. La cual consiste en llevar nuestro texto a una forma m√°s consistente a lo largo del conjunto de datos.\n",
    "\n",
    "Podemos observar que nuestro conjunto de datos cuenta con una alta inconsistencia respecto al uso de may√∫sculas, el uso de tildes y el uso de signos de puntuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUSI√ìN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR',\n",
       " 'OBTRUCCION FOSA NASAL DERECHA',\n",
       " 'Perturbaci√≥n de la actividad y de la atenci√≥n Trastorno def√≠cit atencional',\n",
       " 'M7 PROLAPSO VAGINAL PARED ANTERIOR G11 G 111 ALGIA PELVICA HTA CRONICA',\n",
       " 'PIEZA 3 CARIES DENTINARIA PROFUNDA PROXIMA A CAMARA PULPAR, EVALUAR POR ESPECIALIDAD',\n",
       " 'pieza n 3.4 tratada endodonticamente, restaurada con ionomero y resina compuesta. Necesita protesis fija por gran pNrdida coronaria',\n",
       " 'PZ. 12 TREPANADA',\n",
       " 'CARCINOMA TORIODEO',\n",
       " 'DISPEPSIA Y METEORISMO',\n",
       " 'ASA 1 DENTICION TEMPORAL MORDIDA CRUZADA']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics[\"train\"][\"text\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder llevar todo a min√∫sculas, simplemente podemos utilizar el m√©todo str.lower()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- anomal√≠as dentofaciales (incluso la maloclusi√≥n)\\n\\n\\n discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence_lower = spanish_diagnostics[\"train\"][\"text\"][0].lower()\n",
    "sample_sentence_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar todo los caracteres no alfab√©ticos podemos utilizar un patr√≥n de expresi√≥n regular con la sintaxis: `[^a-z√±√°√©√≠√≥√∫]`, la cual se explica como:\n",
    "\n",
    "- `^`: Este es un `NO` l√≥gico que invierte todo lo que viene a su derecha.\n",
    "- `a-z`: Este patr√≥n coincide todos los caracteres de la `a` a la `z` (min√∫sculas)\n",
    "- `√°√©√≠√≥√∫`: Este patr√≥n coincide con todas las vocales con tilde.\n",
    "\n",
    "Todos estos patrones est√°n concatenados con un `O` l√≥gico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  anomal√≠as dentofaciales  incluso la maloclusi√≥n     discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence_lower_alpha = re.sub(r'[^a-z√±√°√©√≠√≥√∫]', ' ', sample_sentence_lower)\n",
    "sample_sentence_lower_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazamos todas las vocales con tilde con con su forma sin tilde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  anomal√≠as dentofaciales  incluso la maloclusion     discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('√≥', 'o', sample_sentence_lower_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupamos todo en una funci√≥n que normalizar√° una cadena de texto que le pasemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text, remove_tildes = True):\n",
    "    \"\"\"Normaliza una cadena de texto convirti√©ndo todo a min√∫sculas, quitando los caracteres no alfab√©ticos y los tildes\"\"\"\n",
    "    text = text.lower() # Llevamos todo a min√∫scula\n",
    "    text = re.sub(r'[^A-Za-z√±√°√©√≠√≥√∫]', ' ', text) # Reemplazamos los caracteres no alfab√©ticos por un espacio\n",
    "    if remove_tildes:\n",
    "        text = re.sub('√°', 'a', text) # Reemplazamos los tildes\n",
    "        text = re.sub('√©', 'e', text)\n",
    "        text = re.sub('√≠', 'i', text)\n",
    "        text = re.sub('√≥', 'o', text)\n",
    "        text = re.sub('√∫', 'u', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetos del tipo Dataset implementan un m√©todo Dataset.map() con el cual podemo aplicar uan funci√≥n a cada una de las instancias de nuesto conjunto de datos. Lo interesante de este m√©todo es que aplica la funci√≥n de manera paralela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2395fc88a1924c6480a5421507ddba1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=70000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spanish_diagnostics_normalized = spanish_diagnostics[\"train\"].map(\n",
    "    lambda x: { # Utilizamos una funci√≥n an√≥nima que devuelve un diccionario\n",
    "        \"normalized_text\" : normalize(x[\"text\"]) # Esta es una nueva caracter√≠stica que agregaremos a nuestro conjunto de datos.\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora nuestro conjunto de datos cuenta con una nueva caracter√≠stica `normalized_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'normalized_text': '  anomalias dentofaciales  incluso la maloclusion     discrepancia dentomaxilar',\n",
       " 'text': '- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUSI√ìN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizaci√≥n\n",
    "\n",
    "La tokenizaci√≥n es el proceso de demarcaci√≥n de secciones de una cadena de caracteres. Estas secciones podr√≠an ser oraciones, palabras o subpalabras.\n",
    "\n",
    "El m√©todo m√°s simple para tokenizar una cadena de caracteres en nuestro lenguaje es la separaci√≥n por espacios. Aplicamos una separaci√≥n por espacios mediante el m√©todo `str.split()` sobre nuestro conjunto de datos normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  anomalias dentofaciales  incluso la maloclusion     discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"normalized_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anomalias',\n",
       " 'dentofaciales',\n",
       " 'incluso',\n",
       " 'la',\n",
       " 'maloclusion',\n",
       " 'discrepancia',\n",
       " 'dentomaxilar']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"normalized_text\"].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien el m√©todo de separaci√≥n por espacios funciona bien en nuestro conjunto de datos normalizado, tambi√©n quisi√©ramos tokenizar nuestro texto sin normalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUSI√ìN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " 'ANOMAL√çAS',\n",
       " 'DENTOFACIALES',\n",
       " '(INCLUSO',\n",
       " 'LA',\n",
       " 'MALOCLUSI√ìN)',\n",
       " 'DISCREPANCIA',\n",
       " 'DENTOMAXILAR']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"text\"].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aplicar el mismo m√©todos podemos observar que no funciona totalmente bien debido a la presencia de caracteres no alfab√©ticos. Para solucionar esto, existen m√©todos basados en una serie de reglas para solucionar estos problemas. Utilizaremos la implementaci√≥n de un tokenizador basado en reglas de la biblioteca de procesamiento de lenguaje natural Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = es_core_news_sm.load().tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-,\n",
       " ANOMAL√çAS,\n",
       " DENTOFACIALES,\n",
       " (,\n",
       " INCLUSO,\n",
       " LA,\n",
       " MALOCLUSI√ìN,\n",
       " ),\n",
       " \n",
       " \n",
       " \n",
       "  ,\n",
       " DISCREPANCIA,\n",
       " DENTOMAXILAR]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spacy_tokenizer(spanish_diagnostics_normalized[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar el tokenizador basado en reglas, podemos tener resultados mucho mejores que los anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ó Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ó tambi√©n cuenta con una biblioteca llamada Tokenizers, con la cual podemos construir nuestro tokenizador basado en nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el tokenizador con un modelo WordPiece, el cual parte construyendo un vocabulario que incluye todas los caracteres presentes en el conjunto de datos y posteriormente comienza a mezclar caracteres hasta encontrar conjuntos de caracteres que tienen m√°s probabilidad de aparecer juntos que separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.WordPiece())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta biblioteca nos permite a√±adir pasos de normalizaci√≥n directamente. Replicamos lo mismo que hacemos con nuestra funci√≥n `normalizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tokenizers.normalizers.Sequence([\n",
    "    tokenizers.normalizers.Lowercase(), # Llevamos todo a min√∫scula\n",
    "    tokenizers.normalizers.NFD(), # Separamos cada caracter seg√∫n los elementos que lo componen: √° -> (a, ¬¥)\n",
    "    tokenizers.normalizers.StripAccents(), # Eliminamos todos los acentos\n",
    "    tokenizers.normalizers.Replace(tokenizers.Regex(r\"[^a-z ]\"), \" \") # Reemplazamos todos los caracteres no alfab√©ticos\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  anomalias dentofaciales  incluso la maloclusion     discrepancia dentomaxilar'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(spanish_diagnostics_normalized[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A√±adimos el normalizador al tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre tokenizamos nuestro conjunto de datos mediante espacio para delimitar el tama√±o que puede tener cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el entrenador que entrenar√° nuestro tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tokenizers.trainers.WordPieceTrainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el tokenizador sobre nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(spanish_diagnostics_normalized[\"text\"], trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el m√©todo `Tokenizer.encode()` obtenemos la representaci√≥n tokenizada de nuesto texto. Esta representaci√≥n contiene varios atributos, donde los m√°s interesantes son:\n",
    "\n",
    "- `ids`: Contiene nuestro texto representado a trav√©s de una lista que contiene los identificadores de cada token.\n",
    "- `tokens`: Contiene nuestro texto representado a trav√©s de una lista que contiene el texto de cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUSI√ìN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_output = tokenizer.encode(spanish_diagnostics_normalized[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[383, 618, 635, 119, 466, 1765, 970]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anomalias',\n",
       " 'dentofaciales',\n",
       " 'incluso',\n",
       " 'la',\n",
       " 'maloclusion',\n",
       " 'discrepancia',\n",
       " 'dentomaxilar']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_output.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como lo hicimos anteriormente podemos aplicar paralelamente nuestro tokenizador sobre el conjunto de datos mediante el m√©todo `Dataset.map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18458142d13247598c70855572887be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=70000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spanish_diagnostics_normalized_tokenized = spanish_diagnostics_normalized.map(lambda x: {\"tokenized_text\":tokenizer.encode(x[\"text\"]).tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro conjunto de datos ahora contiene el texto tokenizado en la caracter√≠stica `tokenized_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'normalized_text': '  anomalias dentofaciales  incluso la maloclusion     discrepancia dentomaxilar',\n",
       " 'text': '- ANOMAL√çAS DENTOFACIALES (INCLUSO LA MALOCLUSI√ìN)\\n\\n\\n DISCREPANCIA DENTOMAXILAR',\n",
       " 'tokenized_text': ['anomalias',\n",
       "  'dentofaciales',\n",
       "  'incluso',\n",
       "  'la',\n",
       "  'maloclusion',\n",
       "  'discrepancia',\n",
       "  'dentomaxilar']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_diagnostics_normalized_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming y Lematizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de disminuir la cantidad de caracter√≠sticas de las representaciones de texto existen m√©todos que reducen el tama√±o de vocabulario al eliminar inflexiones que puedan tener las palabras. Estos m√©todos son:\n",
    "\n",
    "- Lematizaci√≥n: Este m√©todo lleva una palabra en su forma flexionada a su forma base, por ejemplo *tratada* -> *tratar*\n",
    "- Stemming: Este m√©todo trunca las palabras de entrada mediante un algoritmo predefinido para encontrar la ra√≠z de la misma, por ejemplo *tratada* -> *trat*\n",
    "\n",
    "El proceso de lematizaci√≥n lo haremos a trav√©s de la biblioteca Spacy y el proceso de stemming a trav√©s de la biblioteca NLTK utilizando el algoritmo Snowball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el analizador de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = es_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos como tokenizador el que entrenamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    tokens = tokenizer.encode(text).tokens\n",
    "    return spacy.tokens.Doc(nlp.vocab,tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = custom_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar c√≥mo funcionan estos m√©todos sobre un texto de prueba de nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: pieza\n",
      "Lema: pieza\n",
      "Stem: piez\n",
      "---\n",
      "Token: n\n",
      "Lema: n\n",
      "Stem: n\n",
      "---\n",
      "Token: tratada\n",
      "Lema: tratar\n",
      "Stem: trat\n",
      "---\n",
      "Token: endodonticamente\n",
      "Lema: endodonticamente\n",
      "Stem: endodont\n",
      "---\n",
      "Token: restaurada\n",
      "Lema: restaurar\n",
      "Stem: restaur\n",
      "---\n",
      "Token: con\n",
      "Lema: con\n",
      "Stem: con\n",
      "---\n",
      "Token: ionomero\n",
      "Lema: ionomero\n",
      "Stem: ionomer\n",
      "---\n",
      "Token: y\n",
      "Lema: y\n",
      "Stem: y\n",
      "---\n",
      "Token: resina\n",
      "Lema: resinar\n",
      "Stem: resin\n",
      "---\n",
      "Token: compuesta\n",
      "Lema: componer\n",
      "Stem: compuest\n",
      "---\n",
      "Token: necesita\n",
      "Lema: necesitar\n",
      "Stem: necesit\n",
      "---\n",
      "Token: protesis\n",
      "Lema: protesis\n",
      "Stem: protesis\n",
      "---\n",
      "Token: fija\n",
      "Lema: fijo\n",
      "Stem: fij\n",
      "---\n",
      "Token: por\n",
      "Lema: por\n",
      "Stem: por\n",
      "---\n",
      "Token: gran\n",
      "Lema: gran\n",
      "Stem: gran\n",
      "---\n",
      "Token: pnrdida\n",
      "Lema: pnrdida\n",
      "Stem: pnrdid\n",
      "---\n",
      "Token: coronaria\n",
      "Lema: coronario\n",
      "Stem: coronari\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for t in nlp(spanish_diagnostics_normalized_tokenized[5][\"text\"]):\n",
    "    print(f\"Token: {t.text}\\nLema: {t.lemma_}\\nStem: {stemmer.stem(t.text)}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
